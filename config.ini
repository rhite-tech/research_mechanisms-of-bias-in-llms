# replace weights_directory with either:
# * the *ABSOLUTE PATH* to where you store your weights, or
# * the huggingface repo with your weights
[llama-13b]
weights_directory = /scratch-shared/tpungas/llama-13b
name = LLaMA-13B
tokenizer_class = LlamaTokenizer
model_class = LlamaForCausalLM
layers = model.layers
probe_layer = 13
intervene_layer = 7
noperiod = False

[llama-2-13b]
weights_directory = meta-llama/Llama-2-13b-hf
name = LLaMA-2-13B
tokenizer_class = AutoTokenizer
model_class = AutoModelForCausalLM
layers = model.layers
probe_layer = 14
intervene_layer = 8
noperiod = False

[llama-3-8b]
weights_directory = meta-llama/Meta-Llama-3-8B
name = LLaMA-3-8b
tokenizer_class = AutoTokenizer
model_class = AutoModelForCausalLM
layers = model.layers
probe_layer = 13
intervene_layer = 8
noperiod = False

[llama-3-70b]
weights_directory = meta-llama/Meta-Llama-3-70B
name = LLaMA-3-70b
tokenizer_class = AutoTokenizer
model_class = AutoModelForCausalLM
layers = model.layers
probe_layer = 33
intervene_layer = 21
noperiod = False

[gpt-j-6b]
weights_directory = EleutherAI/gpt-j-6b
name = gpt-j-6b
tokenizer_class = AutoTokenizer
model_class = AutoModelForCausalLM
layers = model.layers
probe_layer = 14
intervene_layer = 8
noperiod = False

[gpt2-xl]
weights_directory = openai-community/gpt2-xl
name = gpt2-xl
tokenizer_class = AutoTokenizer
model_class = AutoModelForCausalLM
layers = model.layers
probe_layer = 14
intervene_layer = 8
noperiod = False
# possibly layers = gptj_6b.layers or sth

# Tarmo's models are above

[llama-2-70b]
weights_directory = meta-llama/Llama-2-70b-hf
name = LLaMA-2-13B
tokenizer_class = AutoTokenizer
model_class = AutoModelForCausalLM
layers = model.layers
intervene_layer = 8
probe_layer = 27
noperiod = False

[llama-2-7b]
weights_directory = meta-llama/Llama-2-7b-hf
name = LLaMA-2-7B
tokenizer_class = AutoTokenizer
model_class = AutoModelForCausalLM
layers = model.layers
probe_layer = 13
noperiod = False

[opt-13b]
weights_directory = facebook/opt-13B
name = OPT-13B
tokenizer_class = AutoTokenizer
model_class = OPTForCausalLM
layers = model.decoder.layers
probe_layer = 28
intervene_layer = 22
noperiod = False
[pythia-12b]
weights_directory = EleutherAI/pythia-12b
name = Pythia-12B
tokenizer_class = AutoTokenizer
model_class = GPTNeoXForCausalLM
layers = gpt_neox.layers
probe_layer = 16
intervene_layer = 6
noperiod = False
[neox-20b]
weights_directory = EleutherAI/gpt-neox-20b
name = GPT-NeoX-20B
tokenizer_class = AutoTokenizer
model_class = AutoModelForCausalLM
layers = gpt_neox.layers
intervene_layer = 10
probe_layer = 14
noperiod = True

[llama-2-13b-reset]
weights_directory = meta-llama/Llama-2-13b-hf
name = LLaMA-2-13B-reset
tokenizer_class = AutoTokenizer
model_class = AutoModelForCausalLM
layers = model.layers
probe_layer = 14
intervene_layer = 8
noperiod = False

[hf_key]
hf_key = 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
